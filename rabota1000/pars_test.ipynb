{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from lxml.etree import tostring as htmlstring\n",
    "import requests\n",
    "import psycopg2 as psy\n",
    "\n",
    "from fake_useragent import FakeUserAgent\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "from datetime import date, timedelta\n",
    "import json\n",
    "import time\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузка данных из файла окружения\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Обращения\n",
    "# config['hh_api_name']\n",
    "# config['hh_api_Client_ID']\n",
    "# config['hh_api_Client_Secret']\n",
    "\n",
    "params = {\n",
    "    'grant_type':'client_credentials',\n",
    "    'client_id':config['hh_api_Client_ID'],\n",
    "    'client_secret':config['hh_api_Client_Secret']\n",
    "}\n",
    "\n",
    "access_token = json.loads(requests.post(f'https://hh.ru/oauth/token', params=params).content.decode())['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные регулярные выражения для проекта\n",
    "re_vacancy_id_hh = r'\\/vacancy\\/(\\d+)\\?'\n",
    "re_vacancy_id_rabota = r'\\/vacancy\\/(\\d+)'\n",
    "re_vacancy_id_finder = r'\\/vacancies\\/(\\d+)'\n",
    "re_vacancy_id_zarplata = r'\\/vacancy\\/card\\/id(\\d+)'\n",
    "\n",
    "re_html_tag_remove = r'<[^>]+>'\n",
    "\n",
    "# re.search(re_vacancy_id, string).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83% |###########################################################             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Found 'name'\n",
      "https://api.hh.ru/vacancies/87813512\n",
      "{'description': 'Not Found', 'errors': [{'type': 'not_found'}], 'request_id': '16968816404547ba83282af804719bde'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    }
   ],
   "source": [
    "class Rabota1000_Parser:\n",
    "    # Класс для парсинга вакансий с ресурса Rabota1000.ru\n",
    "    def __init__(self, city:str='russia'):\n",
    "        self.pre_resualt = []\n",
    "        self.max_page_count = 5\n",
    "        self.basic_url = 'https://rabota1000.ru/russia/'\n",
    "        self.vac_name_list = []\n",
    "        self.vac_name_list = [\n",
    "            'data+scientist', 'data+science', 'дата+сайентист',\n",
    "            'младший+дата+сайентист', 'стажер+дата+сайентист',\n",
    "            'machine+learning', 'ml', 'ml+engineer',\n",
    "            'инженер+машинного+обучения', 'data+engineering',\n",
    "            'инженер+данных', 'младший+инженер+данных',\n",
    "            'junior+data+analyst', 'junior+data+scientist',\n",
    "            'junior+data+engineer', 'data+analyst',\n",
    "            'data+analytics','аналитик+данных', 'big+data+junior'\n",
    "        ]\n",
    "        \n",
    "        ua = FakeUserAgent()\n",
    "        headers = {'user-agent':ua.random}\n",
    "\n",
    "    def pars(self):\n",
    "        if not os.path.exists('pars_link.csv'):\n",
    "            with open('pars_link.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                names = ['vac_name', 'link', 'source', 'vac_id']\n",
    "                file_writer = csv.DictWriter(csv_file, delimiter = \",\", lineterminator=\"\\r\", fieldnames=names)\n",
    "                file_writer.writeheader()\n",
    "\n",
    "            for vac_name in self.vac_name_list:\n",
    "                print(vac_name)\n",
    "                try:\n",
    "                    used_url = self.basic_url + vac_name + \"/\"\n",
    "                    response = requests.get(used_url)\n",
    "                    response.raise_for_status()\n",
    "                    for i in range(1, self.max_page_count+1):\n",
    "                        print(i, end=' ')\n",
    "                        used_url = f'{self.basic_url}{vac_name}?p={i}'\n",
    "                        page = requests.get(used_url)\n",
    "                        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "                        # 20 ссылок на одной странице\n",
    "                        links = [requests.get(link['href']).url for link in soup.findAll('a', attrs={'@click':'vacancyLinkClickHandler'})]\n",
    "                        sources = [source.text for source in soup.findAll('span', attrs={'class':'text-sky-600'})]\n",
    "\n",
    "                        links_to_save = [[vac_name, link, source] for link, source in zip(links, sources)]\n",
    "\n",
    "                        with open('pars_link.csv', 'a', newline='', encoding='utf-8') as csv_file:\n",
    "                            writer = csv.writer(csv_file)\n",
    "                            writer.writerows(links_to_save)\n",
    "                            \n",
    "                except HTTPError as exc:\n",
    "                    code = exc.response.status_code\n",
    "                    print(code)\n",
    "                print()\n",
    "        if not os.path.exists('finaly.csv'):\n",
    "            links_for_processing = []\n",
    "            with open('pars_link.csv', 'r', encoding='utf-8') as csv_file:\n",
    "                reader = csv.reader(csv_file)\n",
    "                labels = next(reader, None)\n",
    "                for row in reader:\n",
    "                    links_for_processing.append(dict(zip(labels, row)))\n",
    "\n",
    "            for item in links_for_processing:\n",
    "                if item['source'] == 'hh.ru':\n",
    "                    item['vac_id'] = re.search(re_vacancy_id_hh, item['link']).group(1)\n",
    "                elif item['source'] == 'finder.vc':\n",
    "                    item['vac_id'] = re.search(re_vacancy_id_finder, item['link']).group(1)\n",
    "                elif item['source'] == 'zarplata.ru':\n",
    "                    item['vac_id'] = re.search(re_vacancy_id_zarplata, item['link']).group(1)\n",
    "                else:\n",
    "                    item['vac_id'] = re.search(re_vacancy_id_rabota, item['link']).group(1)\n",
    "\n",
    "            \n",
    "            bar = progressbar.ProgressBar(maxval=len(links_for_processing)).start()\n",
    "            k = 0\n",
    "            for link in links_for_processing:\n",
    "                if link['source'] == 'hh.ru':\n",
    "                    self.pre_resualt.append(self._pars_url_hh(link['vac_id']))\n",
    "                elif link['source'] == 'zarplata.ru':\n",
    "                    self.pre_resualt.append(self._pars_url_zarplata(link['vac_id']))\n",
    "                elif link['source'] == 'finder.vc':\n",
    "                    self.pre_resualt.append(self._pars_url_finder(link['vac_id']))\n",
    "                else:\n",
    "                    self.pre_resualt.append(self._pars_url_other(link['vac_id']))\n",
    "                k += 1\n",
    "                bar.update(k)\n",
    "            \n",
    "            self._save_frame_to_csv()\n",
    "        else:\n",
    "            with open('finaly.csv', 'r', encoding='utf-8') as csv_file:\n",
    "                reader = csv.reader(csv_file)\n",
    "                labels = next(reader, None)\n",
    "                for row in reader:\n",
    "                    self.pre_resualt.append(dict(zip(labels, row)))\n",
    "\n",
    "\n",
    "    def get_vac_name_into_file(self, vac_file_path:str)->list:\n",
    "        vac_name_list = []\n",
    "        with open(vac_file_path, mode='r', encoding=\"utf-8\") as file_vac:\n",
    "            vac_name_list = list(map(lambda x: x.lower().replace('\\n', '').replace(' ','+'), file_vac.readlines()))\n",
    "\n",
    "        return vac_name_list\n",
    "\n",
    "    def _pars_url_hh(self, id:str)->dict:\n",
    "        res = {}\n",
    "        try:\n",
    "            data = requests.get(f'https://api.hh.ru/vacancies/{id}', headers = {'Authorization': f'Bearer {access_token}'}).json()\n",
    "            res['vac_link'] = f'https://hh.ru/vacancy/{id}'                             # Ссылка\n",
    "            res['name'] = data['name']                                                  # Название\n",
    "            res['city'] = data['area']['name']                                          # Город\n",
    "            res['company'] = data['employer']['name']                                   # Назвнание компании публикующей вакансию\n",
    "            res['experience'] = data['experience']['name']                              # Опыт работы (нет замены на jun mid и sin)\n",
    "            res['schedule'] = data['schedule']['name']                                  # Тип работы (офис/удаленка и тд)\n",
    "            res['employment'] = data['employment']['name']                              # График работы\n",
    "            res['skills'] = [item['name'] for item in data['key_skills']]               # Ключевые навыки\n",
    "            res['description'] = re.sub(re_html_tag_remove, '', data['description'])    # Полное описание (html теги не убраны)\n",
    "            if data['salary'] == None: \n",
    "                res['salary'] = 'Договорная'                                            # Если ЗП не указано то пишем договорная\n",
    "            else:\n",
    "                res['salary'] = data['salary']                                          # Если есть то берем как есть\n",
    "            res['time'] = data['published_at']                                          # Дата и время публикации\n",
    "        except Exception as e:\n",
    "            print(f'Not Found {e}')\n",
    "            print(f'https://api.hh.ru/vacancies/{id}')\n",
    "            data = requests.get(f'https://api.hh.ru/vacancies/{id}', headers = {'Authorization': f'Bearer {access_token}'}).json()\n",
    "            print(data)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_zarplata(self, id:str)->dict:\n",
    "        res = {}\n",
    "        try:\n",
    "            data = requests.get(f'https://api.zarplata.ru/vacancies/{id}').json()\n",
    "            res['vac_link'] = f'https://www.zarplata.ru/vacancy/card/id{id}'            # Ссылка\n",
    "            res['name'] = data['name']                                                  # Название\n",
    "            res['city'] = data['area']['name']                                          # Город\n",
    "            res['company'] = data['employer']['name']                                   # Назвнание компании публикующей вакансию\n",
    "            res['experience'] = data['experience']['name']                              # Опыт работы (нет замены на jun mid и sin)\n",
    "            res['schedule'] = data['schedule']['name']                                  # Тип работы (офис/удаленка и тд)\n",
    "            res['employment'] = data['employment']['name']                              # График работы\n",
    "            res['skills'] = [item['name'] for item in data['key_skills']]               # Ключевые навыки\n",
    "            res['description'] = re.sub(re_html_tag_remove, '', data['description'])    # Полное описание\n",
    "            if data['salary'] == None: \n",
    "                res['salary'] = 'Договорная'                                            # Если ЗП не указано то пишем договорная\n",
    "            else:\n",
    "                res['salary'] = data['salary']                                          # Если есть то берем как есть\n",
    "            res['time'] = data['published_at']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Not Found {e}')\n",
    "            print(f'https://api.zarplata.ru/vacancies/{id}')\n",
    "            data = requests.get(f'https://api.zarplata.ru/vacancies/{id}').json()\n",
    "            print(data)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_other(self, id:str)->dict:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://rabota1000.ru/vacancy/{id}').text, 'html.parser')\n",
    "        dom = lxml.etree.HTML(str(soup)) \n",
    "        res['vac_link'] = f'https://rabota1000.ru/vacancy/{id}'                                                                                             # Ссылка\n",
    "        res['name'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[1]/h2')[0].text.replace('\\n', '').lstrip().rstrip()            # Название\n",
    "        res['city'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[3]/p[2]/span')[0].text                                         # Город (НЕТ)\n",
    "        res['company'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[3]/p[1]')[0].text.replace('\\n', '').lstrip().rstrip()       # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = ''                                                                                                                              # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = ''                                                                                                                                # Тип работы (офис/удаленка и тд) (НЕТ)\n",
    "        res['employment'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[3]/ul/li[2]/span')[0].text                                      # График работы\n",
    "        res['skills'] = ''                                                                                                                                  # Ключевые навыки\n",
    "        res['description'] = re.sub(re_html_tag_remove, '', dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[4]')[0].text)                                                   # Полное описание (НЕТ)\n",
    "        if len(dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[2]/span'))>0:\n",
    "            res['salary'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[2]/span')[0].text.replace('\\n', '').lstrip().rstrip()        # ЗП\n",
    "        else:\n",
    "            res['salary'] = 'Договорная'\n",
    "        res['time'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[3]/ul/li[1]/span')[0].text.replace('\\n', '').lstrip().rstrip()        # Дата публикации\n",
    "\n",
    "        return res\n",
    "        \n",
    "    def _pars_url_finder(self, id:str)->list:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://finder.vc/vacancies/{id}').text, 'html.parser')\n",
    "        dom = lxml.etree.HTML(str(soup)) \n",
    "        res['vac_link'] = f'https://finder.vc/vacancies/{id}' # Ссылка\n",
    "        res['name'] = soup.find('h1', attrs={'class':'vacancy-info-header__title'}).text # Название\n",
    "        res['city'] = ''              # Город (НЕТ)\n",
    "        res['company'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/a')[0].text        # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[3]/div[1]/div[2]/div')[0].text  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = ''     # Тип работы (офис/удаленка и тд) (НЕТ\n",
    "        res['employment'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[3]/div/div[2]/a')[0].text # График работы\n",
    "        res['skills'] = [li.text for li in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[3]/div[1]/div[2]/div[1]/ul')[0]]           # Ключевые навыки\n",
    "        res['description'] = ''    # Полное описание (НЕТ)\n",
    "        res['salary'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[2]/div[2]/div')[0].text.replace(u'\\xa0', '')\n",
    "\n",
    "        if 'сегодня' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today())\n",
    "        elif 'вчера' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today() - timedelta(days=1))\n",
    "        else:\n",
    "            res['time'] = str(date.today() - timedelta(days=int(re.search(r'Опубликована (\\d+)', dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text).group(1))))\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def _save_frame_to_csv(self):\n",
    "        keys = self.pre_resualt[0].keys()\n",
    "\n",
    "        with open('finaly.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys, delimiter = \",\", lineterminator=\"\\r\")\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(self.pre_resualt)\n",
    "\n",
    "    \n",
    "\n",
    "parser = Rabota1000_Parser()\n",
    "parser.pars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vac_link</th>\n",
       "      <th>name</th>\n",
       "      <th>city</th>\n",
       "      <th>company</th>\n",
       "      <th>experience</th>\n",
       "      <th>schedule</th>\n",
       "      <th>employment</th>\n",
       "      <th>skills</th>\n",
       "      <th>description</th>\n",
       "      <th>salary</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://hh.ru/vacancy/84462656</td>\n",
       "      <td>Data Scientist (NLP)</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Samokat.tech</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>['Python', 'ML', 'Machine Learning', 'Jupyter'...</td>\n",
       "      <td>Для эффективной работы сервисов мы активно исп...</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>2023-09-10T17:07:29+0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://hh.ru/vacancy/86788792</td>\n",
       "      <td>Data scientist</td>\n",
       "      <td>Москва</td>\n",
       "      <td>МегаФон</td>\n",
       "      <td>От 1 года до 3 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>['PyTorch', 'Spark', 'Big Data', 'GAN']</td>\n",
       "      <td>Мы в поисках Middle Data scientist в нашу кома...</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>2023-09-15T16:19:16+0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://hh.ru/vacancy/86664203</td>\n",
       "      <td>Data scientist (Middle)</td>\n",
       "      <td>Москва</td>\n",
       "      <td>МегаФон</td>\n",
       "      <td>От 1 года до 3 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>['Python', 'SQL', 'Spark', 'Hadoop', 'Pandas',...</td>\n",
       "      <td>Привет, ищем в нашу команду Middle Data scient...</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>2023-09-13T12:48:33+0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://hh.ru/vacancy/86597536</td>\n",
       "      <td>Аналитик данных / Data Scientist, Middle, авиа</td>\n",
       "      <td>Москва</td>\n",
       "      <td>Туту.ру</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Полный день</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>['Python', 'SQL', 'Data Analysis', 'Математиче...</td>\n",
       "      <td>Привет! Мы в tutu продаём билеты, чтобы отправ...</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>2023-09-12T11:46:39+0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://hh.ru/vacancy/86336141</td>\n",
       "      <td>Data Scientist (Распознавание речи)</td>\n",
       "      <td>Москва</td>\n",
       "      <td>АБК</td>\n",
       "      <td>От 3 до 6 лет</td>\n",
       "      <td>Удаленная работа</td>\n",
       "      <td>Полная занятость</td>\n",
       "      <td>['RND', 'Python', 'Linux', 'Git', 'SQL', 'PyTo...</td>\n",
       "      <td>Кто мы? Наша компания основана Сбером в 2013 г...</td>\n",
       "      <td>Договорная</td>\n",
       "      <td>2023-09-18T09:01:17+0300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         vac_link  \\\n",
       "0  https://hh.ru/vacancy/84462656   \n",
       "1  https://hh.ru/vacancy/86788792   \n",
       "2  https://hh.ru/vacancy/86664203   \n",
       "3  https://hh.ru/vacancy/86597536   \n",
       "4  https://hh.ru/vacancy/86336141   \n",
       "\n",
       "                                             name    city       company  \\\n",
       "0                            Data Scientist (NLP)  Москва  Samokat.tech   \n",
       "1                                  Data scientist  Москва       МегаФон   \n",
       "2                         Data scientist (Middle)  Москва       МегаФон   \n",
       "3  Аналитик данных / Data Scientist, Middle, авиа  Москва       Туту.ру   \n",
       "4             Data Scientist (Распознавание речи)  Москва           АБК   \n",
       "\n",
       "           experience          schedule        employment  \\\n",
       "0       От 3 до 6 лет  Удаленная работа  Полная занятость   \n",
       "1  От 1 года до 3 лет  Удаленная работа  Полная занятость   \n",
       "2  От 1 года до 3 лет  Удаленная работа  Полная занятость   \n",
       "3       От 3 до 6 лет       Полный день  Полная занятость   \n",
       "4       От 3 до 6 лет  Удаленная работа  Полная занятость   \n",
       "\n",
       "                                              skills  \\\n",
       "0  ['Python', 'ML', 'Machine Learning', 'Jupyter'...   \n",
       "1            ['PyTorch', 'Spark', 'Big Data', 'GAN']   \n",
       "2  ['Python', 'SQL', 'Spark', 'Hadoop', 'Pandas',...   \n",
       "3  ['Python', 'SQL', 'Data Analysis', 'Математиче...   \n",
       "4  ['RND', 'Python', 'Linux', 'Git', 'SQL', 'PyTo...   \n",
       "\n",
       "                                         description      salary  \\\n",
       "0  Для эффективной работы сервисов мы активно исп...  Договорная   \n",
       "1  Мы в поисках Middle Data scientist в нашу кома...  Договорная   \n",
       "2  Привет, ищем в нашу команду Middle Data scient...  Договорная   \n",
       "3  Привет! Мы в tutu продаём билеты, чтобы отправ...  Договорная   \n",
       "4  Кто мы? Наша компания основана Сбером в 2013 г...  Договорная   \n",
       "\n",
       "                       time  \n",
       "0  2023-09-10T17:07:29+0300  \n",
       "1  2023-09-15T16:19:16+0300  \n",
       "2  2023-09-13T12:48:33+0300  \n",
       "3  2023-09-12T11:46:39+0300  \n",
       "4  2023-09-18T09:01:17+0300  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"finaly.csv\")  \n",
    "df[['vac_link', 'name', 'city', 'company', 'experience', 'schedule', 'employment', 'skills', 'description', 'salary']] = df[['vac_link', 'name', 'city', 'company', 'experience', 'schedule', 'employment', 'skills', 'description', 'salary']].astype(\"string\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
