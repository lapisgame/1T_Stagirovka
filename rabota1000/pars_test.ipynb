{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from lxml.etree import tostring as htmlstring\n",
    "import requests\n",
    "\n",
    "from fake_useragent import FakeUserAgent\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "from datetime import date, timedelta\n",
    "import json\n",
    "import time\n",
    "\n",
    "import progressbar\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузка данных из файла окружения\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Обращения\n",
    "# config['hh_api_name']\n",
    "# config['hh_api_Client_ID']\n",
    "# config['hh_api_Client_Secret']\n",
    "\n",
    "params = {\n",
    "    'grant_type':'client_credentials',\n",
    "    'client_id':config['hh_api_Client_ID'],\n",
    "    'client_secret':config['hh_api_Client_Secret']\n",
    "}\n",
    "\n",
    "access_token = json.loads(requests.post(f'https://hh.ru/oauth/token', params=params).content.decode())['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные регулярные выражения для проекта\n",
    "re_vacancy_id_hh = r'\\/vacancy\\/(\\d+)\\?'\n",
    "re_vacancy_id_rabota = r'\\/vacancy\\/(\\d+)'\n",
    "re_vacancy_id_finder = r'\\/vacancies\\/(\\d+)'\n",
    "re_vacancy_id_zarplata = r'\\/vacancy\\/card\\/id(\\d+)'\n",
    "\n",
    "# re.search(re_vacancy_id, string).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88% |###############################################################         |\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83% |###########################################################             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'name'\n",
      "https://api.hh.ru/vacancies/87813512\n",
      "{'description': 'Not Found', 'errors': [{'type': 'not_found'}], 'request_id': '1696873186011518415a1d6b314b0c3f'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    }
   ],
   "source": [
    "class Rabota1000_Parser:\n",
    "    # Класс для парсинга вакансий с ресурса Rabota1000.ru\n",
    "    def __init__(self, city:str='russia'):\n",
    "        self.pre_resualt = []\n",
    "        self.max_page_count = 5\n",
    "        self.basic_url = 'https://rabota1000.ru/russia/'\n",
    "        self.vac_name_list = []\n",
    "        self.vac_name_list = [\n",
    "            'data+scientist', 'data+science', 'дата+сайентист',\n",
    "            'младший+дата+сайентист', 'стажер+дата+сайентист',\n",
    "            'machine+learning', 'ml', 'ml+engineer',\n",
    "            'инженер+машинного+обучения', 'data+engineering',\n",
    "            'инженер+данных', 'младший+инженер+данных',\n",
    "            'junior+data+analyst', 'junior+data+scientist',\n",
    "            'junior+data+engineer', 'data+analyst',\n",
    "            'data+analytics','аналитик+данных', 'big+data+junior'\n",
    "        ]\n",
    "        \n",
    "        ua = FakeUserAgent()\n",
    "        headers = {'user-agent':ua.random}\n",
    "\n",
    "    def pars(self):\n",
    "        if not os.path.exists('pars_link.csv'):\n",
    "            with open('pars_link.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                names = ['vac_name', 'link', 'source', 'vac_id']\n",
    "                file_writer = csv.DictWriter(csv_file, delimiter = \",\", lineterminator=\"\\r\", fieldnames=names)\n",
    "                file_writer.writeheader()\n",
    "\n",
    "            for vac_name in self.vac_name_list:\n",
    "                print(vac_name)\n",
    "                try:\n",
    "                    used_url = self.basic_url + vac_name + \"/\"\n",
    "                    response = requests.get(used_url)\n",
    "                    response.raise_for_status()\n",
    "                    for i in range(1, self.max_page_count+1):\n",
    "                        print(i, end=' ')\n",
    "                        used_url = f'{self.basic_url}{vac_name}?p={i}'\n",
    "                        page = requests.get(used_url)\n",
    "                        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "                        # 20 ссылок на одной странице\n",
    "                        links = [requests.get(link['href']).url for link in soup.findAll('a', attrs={'@click':'vacancyLinkClickHandler'})]\n",
    "                        sources = [source.text for source in soup.findAll('span', attrs={'class':'text-sky-600'})]\n",
    "\n",
    "                        links_to_save = [[vac_name, link, source] for link, source in zip(links, sources)]\n",
    "\n",
    "                        with open('pars_link.csv', 'a', newline='', encoding='utf-8') as csv_file:\n",
    "                            writer = csv.writer(csv_file)\n",
    "                            writer.writerows(links_to_save)\n",
    "                            \n",
    "                except HTTPError as exc:\n",
    "                    code = exc.response.status_code\n",
    "                    print(code)\n",
    "                print()\n",
    "\n",
    "        links_for_processing = []\n",
    "        with open('pars_link.csv', 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            labels = next(reader, None)\n",
    "            for row in reader:\n",
    "                links_for_processing.append(dict(zip(labels, row)))\n",
    "\n",
    "        for item in links_for_processing:\n",
    "            if item['source'] == 'hh.ru':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_hh, item['link']).group(1)\n",
    "            elif item['source'] == 'finder.vc':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_finder, item['link']).group(1)\n",
    "            elif item['source'] == 'zarplata.ru':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_zarplata, item['link']).group(1)\n",
    "            else:\n",
    "                item['vac_id'] = re.search(re_vacancy_id_rabota, item['link']).group(1)\n",
    "\n",
    "        \n",
    "        bar = progressbar.ProgressBar(maxval=len(links_for_processing)).start()\n",
    "        k = 0\n",
    "        for link in links_for_processing:\n",
    "            if link['source'] == 'hh.ru':\n",
    "                self.pre_resualt.append(self._pars_url_hh(link['vac_id']))\n",
    "            elif link['source'] == 'zarplata.ru':\n",
    "                self.pre_resualt.append(self._pars_url_zarplata(link['vac_id']))\n",
    "            elif link['source'] == 'finder.vc':\n",
    "                self.pre_resualt.append(self._pars_url_finder(link['vac_id']))\n",
    "            else:\n",
    "                self.pre_resualt.append(self._pars_url_other(link['vac_id']))\n",
    "            k += 1\n",
    "            bar.update(k)\n",
    "\n",
    "    def get_vac_name_into_file(self, vac_file_path:str)->list:\n",
    "        vac_name_list = []\n",
    "        with open(vac_file_path, mode='r', encoding=\"utf-8\") as file_vac:\n",
    "            vac_name_list = list(map(lambda x: x.lower().replace('\\n', '').replace(' ','+'), file_vac.readlines()))\n",
    "\n",
    "        return vac_name_list\n",
    "\n",
    "    def _pars_url_hh(self, id:str)->dict:\n",
    "        res = {}\n",
    "        try:\n",
    "            data = requests.get(f'https://api.hh.ru/vacancies/{id}', headers = {'Authorization': f'Bearer {access_token}'}).json()\n",
    "            res['vac_link'] = f'https://hh.ru/vacancy/{id}'                 # Ссылка\n",
    "            res['name'] = data['name']                                      # Название\n",
    "            res['city'] = data['area']['name']                              # Город\n",
    "            res['company'] = data['employer']['name']                       # Назвнание компании публикующей вакансию\n",
    "            res['experience'] = data['experience']['name']                  # Опыт работы (нет замены на jun mid и sin)\n",
    "            res['schedule'] = data['schedule']['name']                      # Тип работы (офис/удаленка и тд)\n",
    "            res['employment'] = data['employment']['name']                  # График работы\n",
    "            res['skills'] = [item['name'] for item in data['key_skills']]   # Ключевые навыки\n",
    "            res['description'] = data['description']                        # Полное описание (html теги не убраны)\n",
    "            if data['salary'] == None: \n",
    "                res['salary'] = 'Договорная'                                # Если ЗП не указано то пишем договорная\n",
    "            else:\n",
    "                res['salary'] = data['salary']                              # Если есть то берем как есть\n",
    "            res['time'] = data['published_at']                              # Дата и время публикации\n",
    "        except Exception as e:\n",
    "            print(f'Not Found {e}')\n",
    "            print(f'https://api.hh.ru/vacancies/{id}')\n",
    "            data = requests.get(f'https://api.hh.ru/vacancies/{id}', headers = {'Authorization': f'Bearer {access_token}'}).json()\n",
    "            print(data)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_zarplata(self, id:str)->dict:\n",
    "        res = {}\n",
    "        try:\n",
    "            data = requests.get(f'https://api.zarplata.ru/vacancies/{id}').json()\n",
    "            res['vac_link'] = f'https://www.zarplata.ru/vacancy/card/id{id}'# Ссылка\n",
    "            res['name'] = data['name']                                      # Название\n",
    "            res['city'] = data['area']['name']                              # Город\n",
    "            res['company'] = data['employer']['name']                       # Назвнание компании публикующей вакансию\n",
    "            res['experience'] = data['experience']['name']                  # Опыт работы (нет замены на jun mid и sin)\n",
    "            res['schedule'] = data['schedule']['name']                      # Тип работы (офис/удаленка и тд)\n",
    "            res['employment'] = data['employment']['name']                  # График работы\n",
    "            res['skills'] = [item['name'] for item in data['key_skills']]   # Ключевые навыки\n",
    "            res['description'] = data['description']                        # Полное описание (html теги не убраны)\n",
    "            if data['salary'] == None: \n",
    "                res['salary'] = 'Договорная'                                # Если ЗП не указано то пишем договорная\n",
    "            else:\n",
    "                res['salary'] = data['salary']                              # Если есть то берем как есть\n",
    "            res['time'] = data['published_at']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Not Found {e}')\n",
    "            print(f'https://api.zarplata.ru/vacancies/{id}')\n",
    "            data = requests.get(f'https://api.zarplata.ru/vacancies/{id}').json()\n",
    "            print(data)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_other(self, id:str)->dict:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://rabota1000.ru/vacancy/{id}').text, 'html.parser')\n",
    "        dom = lxml.etree.HTML(str(soup)) \n",
    "        res['vac_link'] = f'https://rabota1000.ru/vacancy/{id}'                                                                                             # Ссылка\n",
    "        res['name'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[1]/h2')[0].text.replace('\\n', '').lstrip().rstrip()            # Название\n",
    "        res['city'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[3]/p[2]/span')[0].text                                         # Город (НЕТ)\n",
    "        res['company'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[3]/p[1]')[0].text.replace('\\n', '').lstrip().rstrip()       # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = ''                                                                                                                              # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = ''                                                                                                                                # Тип работы (офис/удаленка и тд) (НЕТ)\n",
    "        res['employment'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[3]/ul/li[2]/span')[0].text                                      # График работы\n",
    "        res['skills'] = ''                                                                                                                                  # Ключевые навыки\n",
    "        res['description'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[4]')[0].text                                                   # Полное описание (НЕТ)\n",
    "        if len(dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[2]/span'))>0:\n",
    "            res['salary'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[1]/div[2]/span')[0].text.replace('\\n', '').lstrip().rstrip()        # ЗП\n",
    "        else:\n",
    "            res['salary'] = 'Договорная'\n",
    "        res['time'] = dom.xpath('/html/body/div[1]/main/div[2]/div/div/div[2]/section[3]/ul/li[1]/span')[0].text.replace('\\n', '').lstrip().rstrip()        # Дата публикации\n",
    "\n",
    "        return res\n",
    "        \n",
    "    def _pars_url_finder(self, id:str)->list:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://finder.vc/vacancies/{id}').text, 'html.parser')\n",
    "        dom = lxml.etree.HTML(str(soup)) \n",
    "        res['vac_link'] = f'https://finder.vc/vacancies/{id}' # Ссылка\n",
    "        res['name'] = soup.find('h1', attrs={'class':'vacancy-info-header__title'}).text # Название\n",
    "        res['city'] = ''              # Город (НЕТ)\n",
    "        res['company'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/a')[0].text        # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[3]/div[1]/div[2]/div')[0].text  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = ''     # Тип работы (офис/удаленка и тд) (НЕТ\n",
    "        res['employment'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[3]/div/div[2]/a')[0].text # График работы\n",
    "        res['skills'] = [li.text for li in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[3]/div[1]/div[2]/div[1]/ul')[0]]           # Ключевые навыки\n",
    "        res['description'] = ''    # Полное описание (НЕТ)\n",
    "        res['salary'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[2]/div[2]/div')[0].text.replace(u'\\xa0', '')\n",
    "\n",
    "        if 'сегодня' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today())\n",
    "        elif 'вчера' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today() - timedelta(days=1))\n",
    "        else:\n",
    "            res['time'] = str(date.today() - timedelta(days=int(re.search(r'Опубликована (\\d+)', dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text).group(1))))\n",
    "\n",
    "        return res\n",
    "    \n",
    "    def save_frame_to_csv(self, file_name='finaly.csv'):\n",
    "        keys = self.pre_resualt[0].keys()\n",
    "\n",
    "        with open(file_name, 'w', newline='', encoding='utf-8') as output_file:\n",
    "            dict_writer = csv.DictWriter(output_file, keys, delimiter = \",\", lineterminator=\"\\r\")\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(self.pre_resualt)\n",
    "\n",
    "    \n",
    "\n",
    "parser = Rabota1000_Parser()\n",
    "parser.pars()\n",
    "parser.save_frame_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо достать в результате\n",
    "\n",
    "- Название вакансии +\n",
    "- Название компании +\n",
    "- З/п +\n",
    "- Формат работы +\n",
    "- Тип занятости +\n",
    "- Навыки (Если можно достать отдельно) +\n",
    "- Описание вакансии (если есть требования или навыки - то отлично, берем их) +\n",
    "- Время публикации +\n",
    "- Ссылка на вакансию тоже должна быть у нас +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.save_frame_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'finaly.csv'\n",
    "keys = parser.pre_resualt[0].keys()\n",
    "\n",
    "with open(file_name, 'w', newline='', encoding='utf-8') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys, delimiter = \",\", lineterminator=\"\\r\")\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(parser.pre_resualt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
