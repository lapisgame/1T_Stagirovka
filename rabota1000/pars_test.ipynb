{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import lxml\n",
    "from lxml.etree import tostring as htmlstring\n",
    "import requests\n",
    "\n",
    "from fake_useragent import FakeUserAgent\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подгрузка данных из файла окружения\n",
    "config = dotenv_values(\".env\")\n",
    "\n",
    "# Обращения\n",
    "# config[hh_api_name]\n",
    "# config[hh_api_Client_ID]\n",
    "# config[hh_api_Client_Secret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные регулярные выражения для проекта\n",
    "re_vacancy_id_hh = r'\\/vacancy\\/(\\d+)\\?'\n",
    "re_vacancy_id_rabota = r'\\/vacancy\\/(\\d+)'\n",
    "re_vacancy_id_finder = r'\\/vacancies\\/(\\d+)'\n",
    "re_vacancy_id_zarplata = r'\\/vacancy\\/card\\/id(\\d+)'\n",
    "\n",
    "# re.search(re_vacancy_id, string).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rabota1000_Parser:\n",
    "    # Класс для парсинга вакансий с ресурса Rabota1000.ru\n",
    "    def __init__(self, city:str='russia'):\n",
    "        self.max_page_count = 5\n",
    "        self.basic_url = 'https://rabota1000.ru/russia/'\n",
    "        self.vac_name_list = []\n",
    "        self.vac_name_list = self.get_vac_name_into_file('vacancy_name_to_pars.txt')\n",
    "        \n",
    "        ua = FakeUserAgent()\n",
    "        headers = {'user-agent':ua.random}\n",
    "\n",
    "        self._pars()\n",
    "\n",
    "    def _pars(self):\n",
    "        if not os.path.exists('pars_link.csv'):\n",
    "            with open('pars_link.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                names = ['vac_name', 'link', 'source', 'vac_id']\n",
    "                file_writer = csv.DictWriter(csv_file, delimiter = \",\", lineterminator=\"\\r\", fieldnames=names)\n",
    "                file_writer.writeheader()\n",
    "\n",
    "            for vac_name in self.vac_name_list:\n",
    "                print(vac_name)\n",
    "                try:\n",
    "                    used_url = self.basic_url + vac_name + \"/\"\n",
    "                    response = requests.get(used_url)\n",
    "                    response.raise_for_status()\n",
    "                    for i in range(1, self.max_page_count+1):\n",
    "                        print(i, end=' ')\n",
    "                        used_url = f'{self.basic_url}{vac_name}?p={i}'\n",
    "                        page = requests.get(used_url)\n",
    "                        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "                        # 20 ссылок на одной странице\n",
    "                        links = [requests.get(link['href']).url for link in soup.findAll('a', attrs={'@click':'vacancyLinkClickHandler'})]\n",
    "                        sources = [source.text for source in soup.findAll('span', attrs={'class':'text-sky-600'})]\n",
    "\n",
    "                        links_to_save = [[vac_name, link, source] for link, source in zip(links, sources)]\n",
    "\n",
    "                        with open('pars_link.csv', 'a', newline='', encoding='utf-8') as csv_file:\n",
    "                            writer = csv.writer(csv_file)\n",
    "                            writer.writerows(links_to_save)\n",
    "                            \n",
    "                except HTTPError as exc:\n",
    "                    code = exc.response.status_code\n",
    "                    print(code)\n",
    "                print()\n",
    "\n",
    "        links_for_processing = []\n",
    "        with open('pars_link.csv', 'r', encoding='utf-8') as csv_file:\n",
    "            reader = csv.reader(csv_file)\n",
    "            labels = next(reader, None)\n",
    "            for row in reader:\n",
    "                links_for_processing.append(dict(zip(labels, row)))\n",
    "        \n",
    "        for item in links_for_processing:\n",
    "            if item['source'] == 'hh.ru':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_hh, item['link']).group(1)\n",
    "            elif item['source'] == 'finder.vc':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_finder, item['link']).group(1)\n",
    "            elif item['source'] == 'zarplata.ru':\n",
    "                item['vac_id'] = re.search(re_vacancy_id_zarplata, item['link']).group(1)\n",
    "            else:\n",
    "                item['vac_id'] = re.search(re_vacancy_id, item['link']).group(1)\n",
    "\n",
    "        pre_resualt = []\n",
    "        for link in links_for_processing:\n",
    "            if link['source'] == 'hh.ru':\n",
    "                pre_resualt.append(self._pars_url_hh(link['vac_id']))\n",
    "            elif link['source'] == 'zarplata.ru':\n",
    "                pre_resualt.append(self._pars_url_zarplata(link['vac_id']))\n",
    "            elif item['source'] == 'finder.vc':\n",
    "                pre_resualt.append(self._pars_url_finder(link['vac_id']))\n",
    "            else:\n",
    "                print(link['link'])\n",
    "                pre_resualt.append(self._pars_url_other(link['vac_id']))\n",
    "\n",
    "    def get_vac_name_into_file(self, vac_file_path:str)->list:\n",
    "        vac_name_list = []\n",
    "        with open(vac_file_path, mode='r', encoding=\"utf-8\") as file_vac:\n",
    "            vac_name_list = list(map(lambda x: x.lower().replace('\\n', '').replace(' ','+'), file_vac.readlines()))\n",
    "\n",
    "        return vac_name_list\n",
    "\n",
    "    def _pars_url_hh(self, id:str)->dict:\n",
    "        res = {}\n",
    "        data = requests.get(f'https://api.hh.ru/vacancies/{id}').json()\n",
    "        res['vac_link'] = f'https://hh.ru/vacancy/{id}' # Ссылка\n",
    "        res['name'] = data['name']                      # Название\n",
    "        res['city'] = data['area']['name']              # Город\n",
    "        res['company'] = data['employer']['name']       # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = data['experience']['name']  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = data['schedule']['name']      # Тип работы (офис/удаленка и тд)\n",
    "        res['employment'] = data['employment']['name']  # График работы\n",
    "        res['skills'] = data['key_skills']              # Ключевые навыки\n",
    "        res['description'] = data['description']        # Полное описание (html теги не убраны)\n",
    "        if data['salary'] == None: \n",
    "            res['salary'] = 'Договорная'                # Если ЗП не указано то пишем договорная\n",
    "        else:\n",
    "            res['salary'] = data['salary']              # Если есть то берем как есть\n",
    "        res['time'] = data['published_at']              # Дата и время публикации\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_zarplata(self, id:str)->dict:\n",
    "        res = {}\n",
    "        data = requests.get(f'https://api.zarplata.ru/vacancies/{id}').json()\n",
    "        res['vac_link'] = f'https://www.zarplata.ru/vacancy/card/id{id}' # Ссылка\n",
    "        res['name'] = data['name']                      # Название\n",
    "        res['city'] = data['area']['name']              # Город\n",
    "        res['company'] = data['employer']['name']       # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = data['experience']['name']  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = data['schedule']['name']      # Тип работы (офис/удаленка и тд)\n",
    "        res['employment'] = data['employment']['name']  # График работы\n",
    "        res['skills'] = data['key_skills']              # Ключевые навыки\n",
    "        res['description'] = data['description']        # Полное описание (html теги не убраны)\n",
    "        if data['salary'] == None: \n",
    "            res['salary'] = 'Договорная'                # Если ЗП не указано то пишем договорная\n",
    "        else:\n",
    "            res['salary'] = data['salary']              # Если есть то берем как есть\n",
    "        res['time'] = data['published_at']\n",
    "        \n",
    "        return res\n",
    "\n",
    "\n",
    "    def _pars_url_other(self, id:str)->dict:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://rabota1000.ru/vacancy/{id}').text, 'html.parser')\n",
    "        res['vac_link'] = f'https://rabota1000.ru/vacancy/{id}' # Ссылка\n",
    "        res['name'] = soup.find('h2').text                      # Название\n",
    "        res['city'] = data['area']['name']              # Город\n",
    "        res['company'] = data['employer']['name']       # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = data['experience']['name']  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = data['schedule']['name']      # Тип работы (офис/удаленка и тд)\n",
    "        res['employment'] = data['employment']['name']  # График работы\n",
    "        res['skills'] = data['key_skills']              # Ключевые навыки\n",
    "        res['description'] = data['description']        # Полное описание (html теги не убраны)\n",
    "        if data['salary'] == None: \n",
    "            res['salary'] = 'Договорная'                # Если ЗП не указано то пишем договорная\n",
    "        else:\n",
    "            res['salary'] = data['salary']              # Если есть то берем как есть\n",
    "        res['time'] = data['published_at']\n",
    "        \n",
    "    def _pars_url_finder(self, id:str)->list:\n",
    "        res = {}\n",
    "        soup = BeautifulSoup(requests.get(f'https://finder.vc/vacancies/{id}').text, 'html.parser')\n",
    "        dom = lxml.etree.HTML(str(soup)) \n",
    "        res['vac_link'] = f'https://finder.vc/vacancies/{id}' # Ссылка\n",
    "        res['name'] = soup.find('h1', attrs={'class':'vacancy-info-header__title'}).text # Название\n",
    "        res['city'] = ''              # Город (НЕТ)\n",
    "        res['company'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[1]/div[2]/div/div[1]/a')[0].text        # Назвнание компании публикующей вакансию\n",
    "        res['experience'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[3]/div[1]/div[2]/div')[0].text  # Опыт работы (нет замены на jun mid и sin)\n",
    "        res['schedule'] = ''     # Тип работы (офис/удаленка и тд) (НЕТ\n",
    "        res['employment'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[3]/div/div[2]/a')[0].text # График работы\n",
    "        res['skills'] = [li.text for li in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[3]/div[1]/div[2]/div[1]/ul')[0]]           # Ключевые навыки\n",
    "        res['description'] = ''    # Полное описание (НЕТ)\n",
    "        res['salary'] = dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[2]/div[2]/div[2]/div')[0].text.replace(u'\\xa0', '')\n",
    "\n",
    "        if 'сегодня' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today())\n",
    "        elif 'вчера' in dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text:\n",
    "            res['time'] = str(date.today() - timedelta(days=1))\n",
    "        else:\n",
    "            res['time'] = str(date.today() - timedelta(days=int(re.search(r'Опубликована (\\d+)', dom.xpath('/html/body/div[1]/div[2]/div/main/div/div/div[2]/div[1]/div/div/div[1]/div/div[1]')[0].text).group(1))))\n",
    "\n",
    "        return res\n",
    "\n",
    "    \n",
    "\n",
    "parser = Rabota1000_Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо достать в результате\n",
    "\n",
    "- Название вакансии +\n",
    "- Название компании +\n",
    "- З/п +\n",
    "- Формат работы +\n",
    "- Тип занятости +\n",
    "- Навыки (Если можно достать отдельно) +\n",
    "- Описание вакансии (если есть требования или навыки - то отлично, берем их) +\n",
    "- Время публикации +\n",
    "- Ссылка на вакансию тоже должна быть у нас +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://rabota1000.ru/vacancy/103050901\n",
      "{'vac_link': 'https://rabota1000.ru/vacancy/103050901', 'name': '\\n                            Младший воспитатель                        ', 'city': 'Москва', 'company': 'Рултек', 'experience': 'Нет опыта', 'schedule': 'Полный день', 'employment': 'Полная занятость', 'skills': [{'name': 'Мобильность'}, {'name': 'Пользователь ПК'}], 'description': '<p><strong>Требуется курьер </strong>на доставку документов</p> <p> </p> <p><strong>Условия:</strong></p> <p>Доставка документов</p> <p>Проезд и мобильная связь оплачивается в полном размере.</p> <p>Зарплата выплачивается регулярно, 2 раза в месяц.</p>', 'salary': {'from': 28000, 'to': 35000, 'currency': 'RUR', 'gross': False}, 'time': '2023-10-05T12:08:57+0300'}\n"
     ]
    }
   ],
   "source": [
    "id = 103050901\n",
    "print(f'https://rabota1000.ru/vacancy/{id}')\n",
    "soup = BeautifulSoup(requests.get(f'https://rabota1000.ru/vacancy/{id}').text, 'html.parser')\n",
    "res['vac_link'] = f'https://rabota1000.ru/vacancy/{id}' # Ссылка\n",
    "res['name'] = soup.find('h2').text                      # Название\n",
    "res['city'] = ''              # Город\n",
    "res['company'] =        # Назвнание компании публикующей вакансию\n",
    "res['experience'] =   # Опыт работы (нет замены на jun mid и sin)\n",
    "res['schedule'] =       # Тип работы (офис/удаленка и тд)\n",
    "res['employment'] =  # График работы\n",
    "res['skills'] =             # Ключевые навыки\n",
    "res['description'] =        # Полное описание (html теги не убраны)\n",
    "res['salary'] = []\n",
    "res['time'] = data['published_at']\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vac_link': 'https://finder.vc/vacancies/32381', 'name': 'Junior data-аналитик', 'city': '', 'company': 'Бизнес-метрика', 'experience': 'Без опыта', 'schedule': '', 'employment': 'Полный день', 'skills': ['Знание SQL или python.', 'Знание теории data моделирования, проектирования аналитических хранилищ.', 'Знание основ статистики.', 'Умение визуализировать информацию и презентовать идеи и гипотезы.', 'Системное и структурное мышление.'], 'description': b'<div class=\"vacancy-info__section\" data-v-2d74938e=\"\"><div class=\"vacancy-info-body vacancy-info__body\" data-v-2d74938e=\"\" data-v-a06ab32e=\"\"><div class=\"vacancy-info-body__description\" data-v-a06ab32e=\"\">&#1053;&#1072;&#1096;&#1080; &#1087;&#1088;&#1086;&#1077;&#1082;&#1090;&#1099; &#1082;&#1083;&#1072;&#1089;&#1089;&#1080;&#1092;&#1080;&#1094;&#1080;&#1088;&#1091;&#1102;&#1090;&#1089;&#1103; &#1086;&#1090; &#1085;&#1077;&#1073;&#1086;&#1083;&#1100;&#1096;&#1080;&#1093; &#1080;&#1085;&#1090;&#1077;&#1075;&#1088;&#1072;&#1094;&#1080;&#1081; &#1076;&#1086; &#1082;&#1088;&#1091;&#1087;&#1085;&#1099;&#1093; &#1079;&#1072;&#1082;&#1072;&#1079;&#1086;&#1074; &#1085;&#1072; &#1089;&#1083;&#1086;&#1078;&#1085;&#1091;&#1102; &#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1082;&#1091;, &#1075;&#1076;&#1077; &#1090;&#1088;&#1077;&#1073;&#1091;&#1077;&#1090;&#1089;&#1103; &#1094;&#1077;&#1083;&#1072;&#1103; &#1076;&#1072;&#1090;&#1072;-&#1082;&#1086;&#1084;&#1072;&#1085;&#1076;&#1072;, &#1074; &#1089;&#1074;&#1103;&#1079;&#1080; &#1089; &#1095;&#1077;&#1084; &#1084;&#1099; &#1080; &#1085;&#1072;&#1073;&#1080;&#1088;&#1072;&#1077;&#1084; &#1085;&#1086;&#1074;&#1099;&#1093; &#1083;&#1102;&#1076;&#1077;&#1081;.</div><div class=\"vacancy-info-body__lists\" data-v-a06ab32e=\"\"><div class=\"vacancy-info-body__info\" data-v-a06ab32e=\"\"><h2 class=\"vacancy-info-body__title\" data-v-a06ab32e=\"\">&#1058;&#1088;&#1077;&#1073;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103;:</h2><ul class=\"vacancy-info-body__list\" data-v-a06ab32e=\"\"><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1047;&#1085;&#1072;&#1085;&#1080;&#1077; SQL &#1080;&#1083;&#1080; python.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1047;&#1085;&#1072;&#1085;&#1080;&#1077; &#1090;&#1077;&#1086;&#1088;&#1080;&#1080; data &#1084;&#1086;&#1076;&#1077;&#1083;&#1080;&#1088;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103;, &#1087;&#1088;&#1086;&#1077;&#1082;&#1090;&#1080;&#1088;&#1086;&#1074;&#1072;&#1085;&#1080;&#1103; &#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1095;&#1077;&#1089;&#1082;&#1080;&#1093; &#1093;&#1088;&#1072;&#1085;&#1080;&#1083;&#1080;&#1097;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1047;&#1085;&#1072;&#1085;&#1080;&#1077; &#1086;&#1089;&#1085;&#1086;&#1074; &#1089;&#1090;&#1072;&#1090;&#1080;&#1089;&#1090;&#1080;&#1082;&#1080;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1059;&#1084;&#1077;&#1085;&#1080;&#1077; &#1074;&#1080;&#1079;&#1091;&#1072;&#1083;&#1080;&#1079;&#1080;&#1088;&#1086;&#1074;&#1072;&#1090;&#1100; &#1080;&#1085;&#1092;&#1086;&#1088;&#1084;&#1072;&#1094;&#1080;&#1102; &#1080; &#1087;&#1088;&#1077;&#1079;&#1077;&#1085;&#1090;&#1086;&#1074;&#1072;&#1090;&#1100; &#1080;&#1076;&#1077;&#1080; &#1080; &#1075;&#1080;&#1087;&#1086;&#1090;&#1077;&#1079;&#1099;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1057;&#1080;&#1089;&#1090;&#1077;&#1084;&#1085;&#1086;&#1077; &#1080; &#1089;&#1090;&#1088;&#1091;&#1082;&#1090;&#1091;&#1088;&#1085;&#1086;&#1077; &#1084;&#1099;&#1096;&#1083;&#1077;&#1085;&#1080;&#1077;.</li></ul></div><div class=\"vacancy-info-body__info\" data-v-a06ab32e=\"\"><h2 class=\"vacancy-info-body__title\" data-v-a06ab32e=\"\">&#1054;&#1073;&#1103;&#1079;&#1072;&#1085;&#1085;&#1086;&#1089;&#1090;&#1080;:</h2><ul class=\"vacancy-info-body__list\" data-v-a06ab32e=\"\"><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1042;&#1077;&#1073;-&#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1082;&#1072;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1052;&#1086;&#1073;&#1080;&#1083;&#1100;&#1085;&#1072;&#1103; &#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1082;&#1072;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1055;&#1088;&#1086;&#1076;&#1091;&#1082;&#1090;&#1086;&#1074;&#1072;&#1103; &#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1082;&#1072;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">BI-&#1072;&#1085;&#1072;&#1083;&#1080;&#1090;&#1080;&#1082;&#1072;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">Data Science.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">Data engineering.</li></ul></div><div class=\"vacancy-info-body__info\" data-v-a06ab32e=\"\"><h2 class=\"vacancy-info-body__title\" data-v-a06ab32e=\"\">&#1059;&#1089;&#1083;&#1086;&#1074;&#1080;&#1103;:</h2><ul class=\"vacancy-info-body__list\" data-v-a06ab32e=\"\"><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1059;&#1076;&#1072;&#1083;&#1077;&#1085;&#1085;&#1099;&#1081; &#1092;&#1086;&#1088;&#1084;&#1072;&#1090; &#1088;&#1072;&#1073;&#1086;&#1090;&#1099;.</li><li class=\"vacancy-info-body__item\" data-v-a06ab32e=\"\">&#1055;&#1086;&#1083;&#1085;&#1099;&#1081; &#1076;&#1077;&#1085;&#1100;.</li></ul></div><!-- --><!-- --></div></div><div class=\"vacancy-info-footer vacancy-info__footer\" data-v-2d74938e=\"\" data-v-8bd41fae=\"\"><!-- --><div class=\"vacancy-info-footer__actions\" data-v-8bd41fae=\"\"><button class=\"vacancy-info-footer__button button button_primary button_bold button_uppercase button_mobile-block\" data-v-8bd41fae=\"\" data-v-f5316d04=\"\"><!-- -->&#1055;&#1086;&#1082;&#1072;&#1079;&#1072;&#1090;&#1100; &#1082;&#1086;&#1085;&#1090;&#1072;&#1082;&#1090;&#1099;</button><!-- --></div></div></div>', 'salary': 'от 60000 ₽', 'time': '2023-09-28'}\n"
     ]
    }
   ],
   "source": [
    "id = 32381\n",
    "\n",
    "\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
